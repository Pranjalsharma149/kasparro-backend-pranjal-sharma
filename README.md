# ğŸš€ Kasparro â€“ Backend & ETL Systems

This repository contains a **production-grade, dockerized Backend & ETL system** built as part of the Kasparro Backend & ETL Systems Assignment. The system demonstrates end-to-end ownership of data ingestion, transformation, persistence, and API exposure, following clean architecture and industry best practices.

---

## ğŸ¯ Project Goals

* Build a **robust ETL pipeline** ingesting data from multiple heterogeneous sources
* Normalize and store data in a **PostgreSQL-backed unified schema**
* Expose a **lightweight, production-ready API**
* Ensure **incremental ingestion, fault tolerance, and observability**
* Deliver a **fully dockerized system** runnable with a single command

---

## ğŸ§± Architecture Overview

```
kasparro-backend-pranjal-sharma/
â”‚
â”œâ”€â”€ api/                # FastAPI routes & controllers
â”œâ”€â”€ ingestion/          # ETL ingestion logic (per source)
â”œâ”€â”€ services/           # Business logic & transformations
â”œâ”€â”€ schemas/            # Pydantic models & validation
â”œâ”€â”€ core/               # DB, config, dependencies, checkpoints
â”œâ”€â”€ tests/              # Automated test suite
â”‚
â”œâ”€â”€ main.py             # Application entrypoint
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

The system follows **separation of concerns**:

* Ingestion layer handles external data fetching
* Service layer handles transformation & normalization
* Core layer handles DB, config, checkpoints, and shared utilities
* API layer exposes validated data and ETL metadata

---

## ğŸ”Œ Data Sources

### P0 â€“ Required Sources

1. **API Source** (Authenticated)

   * Uses provided API key via environment variables
   * Example: CoinGecko / CoinPaprika

2. **CSV Source**

   * Handles schema quirks and type inconsistencies
   * Loaded incrementally into raw tables

### P1 â€“ Additional Source

3. **Third Source**

   * Additional API / CSV / RSS feed
   * Unified into the same normalized schema

All raw ingested data is stored in:

* `raw_api_data`
* `raw_csv_data`
* `raw_third_source_data`

---

## ğŸ”„ ETL Pipeline

### Features

* Incremental ingestion (no reprocessing)
* Checkpoint-based resume-on-failure
* Idempotent writes
* Pydantic-based validation & type cleaning

### ETL Flow

1. Fetch data from source
2. Validate & clean using Pydantic schemas
3. Store raw data (`raw_*` tables)
4. Normalize into unified tables
5. Update checkpoint table
6. Record run metadata

---

## ğŸ§ª Testing Strategy

The project includes an automated test suite covering:

* ETL transformation logic
* Incremental ingestion & checkpoints
* Failure & recovery scenarios
* Schema mismatches
* API endpoints
* Rate limiting logic (if enabled)

Run tests using:

```bash
make test
```

---

## ğŸŒ API Endpoints

### GET /data

Returns normalized data with pagination & filtering.

**Features:**

* Pagination (`limit`, `offset`)
* Filtering
* Metadata returned:

  * `request_id`
  * `api_latency_ms`

---

### GET /health

Reports system health:

* Database connectivity
* Last ETL run status

---

### GET /stats

Provides ETL execution statistics:

* Records processed
* Duration
* Last success timestamp
* Last failure timestamp
* Run metadata

---

## ğŸ³ Dockerized Execution

The entire system runs using Docker.

### Prerequisites

* Docker
* Docker Compose
* Make

### Commands

```bash
make up     # Start ETL + API
make down   # Stop services
make test   # Run tests
```

The Docker image:

* Automatically starts the ETL service
* Exposes API endpoints immediately

---

## ğŸ” Configuration & Secrets

All secrets are handled securely via environment variables:

```env
API_KEY=your_api_key_here
DATABASE_URL=postgresql://...
```

âš ï¸ No secrets are hard-coded.

---

## â˜ï¸ Cloud Deployment

The system is deployed on a cloud provider (AWS / GCP / Azure) with:

* Public API endpoints
* Scheduled ETL runs (cron / scheduler)
* Cloud-native logs & metrics

During evaluation, the following will be demonstrated:

* Cron-triggered ETL runs
* Live logs & metrics via cloud dashboard
* ETL recovery after failure

---

## ğŸ” Observability (Optional Enhancements)

* Structured JSON logs
* ETL run metadata
* Optional `/metrics` endpoint (Prometheus-compatible)

---

## ğŸ§  Design Decisions

* **FastAPI** for performance & validation
* **PostgreSQL** for reliability and schema control
* **Pydantic** for strong data contracts
* **Docker-first** approach for reproducibility
* **Checkpointing** for fault tolerance

---

## âœ… Assignment Coverage

* âœ” P0 â€“ Foundation Layer
* âœ” P1 â€“ Growth Layer
* â­ Partial P2 â€“ Differentiators (where applicable)

This project was built with a strong emphasis on **production readiness**, **clarity**, and **scalability**, closely mirroring real-world backend ownership at Kasparro.

---

## ğŸ™Œ Final Note

This assignment was approached not just as a test, but as a **learning-focused, production-style system build**. Every design choice prioritizes reliability, maintainability, and real-world engineering standards.

**Built with curiosity. Built with clarity. Built to differentiate.** ğŸš€
